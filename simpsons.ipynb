{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:15.527744Z",
     "start_time": "2021-04-25T18:36:15.042636Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will verify that GPU is enabled for this notebook\n",
    "# following should print: CUDA is available!  Training on GPU ...\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:16.483959Z",
     "start_time": "2021-04-25T18:36:15.593760Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from matplotlib import colors, pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:16.562617Z",
     "start_time": "2021-04-25T18:36:16.547613Z"
    }
   },
   "outputs": [],
   "source": [
    "# разные режимы датасета \n",
    "DATA_MODES = ['train', 'val', 'test']\n",
    "# все изображения будут масштабированы к размеру 224x224 px\n",
    "RESCALE_SIZE = 224\n",
    "# работаем на видеокарте\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:17.190661Z",
     "start_time": "2021-04-25T18:36:17.180659Z"
    }
   },
   "outputs": [],
   "source": [
    "# используем враппер над датасетом для удобной работы\n",
    "\n",
    "class SimpsonsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет с картинками, который паралельно подгружает их из папок\n",
    "    производит скалирование и превращение в торчевые тензоры\n",
    "    \"\"\"\n",
    "    def __init__(self, files, mode):\n",
    "        super().__init__()\n",
    "        # список файлов для загрузки\n",
    "        self.files = sorted(files)\n",
    "        # режим работы\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "     \n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file)\n",
    "        image.load()\n",
    "        return image\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            transform = transforms.Compose([\n",
    "                 transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "                 transforms.RandomHorizontalFlip(),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                 ])\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                ])\n",
    "        \n",
    "        \n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = transform(x)\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:18.144665Z",
     "start_time": "2021-04-25T18:36:18.126662Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
    "    \"\"\"Imshow для тензоров\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt_ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt_ax.set_title(title)\n",
    "    plt_ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:20.363071Z",
     "start_time": "2021-04-25T18:36:20.172025Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('journey-springfield/train/simpsons_dataset')\n",
    "TEST_DIR = Path('journey-springfield/testset/testset')\n",
    "\n",
    "train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\n",
    "test_files = sorted(list(TEST_DIR.rglob('*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:21.022119Z",
     "start_time": "2021-04-25T18:36:20.881087Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_labels = [path.parent.name for path in train_val_files]\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n",
    "                                          stratify=train_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:21.546182Z",
     "start_time": "2021-04-25T18:36:21.484170Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = SimpsonsDataset(val_files, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:23.677924Z",
     "start_time": "2021-04-25T18:36:23.658879Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:25.236676Z",
     "start_time": "2021-04-25T18:36:24.464503Z"
    }
   },
   "outputs": [],
   "source": [
    "# Посмотрим на наших героев внутри датасета.\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(8, 8), \\\n",
    "                        sharey=True, sharex=True)\n",
    "for fig_x in ax.flatten():\n",
    "    random_characters = int(np.random.uniform(0,1000))\n",
    "    im_val, label = val_dataset[random_characters]\n",
    "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
    "                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
    "    imshow(im_val.data.cpu(), \\\n",
    "          title=img_label,plt_ax=fig_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T19:44:01.047840Z",
     "start_time": "2021-04-22T19:44:01.031837Z"
    }
   },
   "source": [
    "### Построение нейросети\n",
    "\n",
    "*Описание слоев*:\n",
    "1. размерность входа: 3x224x224 \n",
    "2. размерности после слоя:  8x111x111\n",
    "3. 16x54x54\n",
    "4. 32x26x26\n",
    "5. 64x12x12\n",
    "6. выход: 96x5x5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T10:59:24.684841Z",
     "start_time": "2021-04-25T10:59:24.671837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Очень простая сеть\n",
    "class SimpleCnn(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(96 * 5 * 5, n_classes)\n",
    "  \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:32.351435Z",
     "start_time": "2021-04-25T18:36:32.344434Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:33.784718Z",
     "start_time": "2021-04-25T18:36:33.771716Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:36.558919Z",
     "start_time": "2021-04-25T18:36:36.548917Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_files, val_files, model, epochs, batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        opt = torch.optim.AdamW(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = lr_scheduler.StepLR(opt, step_size=7, gamma=0.1)\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
    "            scheduler.step()\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:37.081966Z",
     "start_time": "2021-04-25T18:36:37.063963Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:39.405236Z",
     "start_time": "2021-04-25T18:36:39.379230Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(train_val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = SimpleCnn(n_classes).to(DEVICE)\n",
    "print(\"we will classify :{}\".format(n_classes))\n",
    "print(simple_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:57.366203Z",
     "start_time": "2021-04-25T18:36:57.180157Z"
    }
   },
   "outputs": [],
   "source": [
    "if val_dataset is None:\n",
    "    val_dataset = SimpsonsDataset(val_files, mode='val')\n",
    "    \n",
    "train_dataset = SimpsonsDataset(train_files, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:36:44.440522Z",
     "start_time": "2021-04-25T18:36:44.127490Z"
    }
   },
   "outputs": [],
   "source": [
    "history = train(train_dataset, val_dataset, model=simple_cnn, epochs=25, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T16:58:50.518625Z",
     "start_time": "2021-04-25T16:58:50.504621Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T11:54:19.849104Z",
     "start_time": "2021-04-25T11:54:19.708073Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка на маленькой сети (AlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T11:56:04.446846Z",
     "start_time": "2021-04-25T11:56:03.126716Z"
    }
   },
   "outputs": [],
   "source": [
    "model_AlexNet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T11:56:05.134001Z",
     "start_time": "2021-04-25T11:56:05.118998Z"
    }
   },
   "outputs": [],
   "source": [
    "model_AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fine Tuning** способ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T11:56:07.082531Z",
     "start_time": "2021-04-25T11:56:07.067528Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 9216\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_AlexNet.classifier = nn.Linear(num_features, n_classes)\n",
    "\n",
    "# Умножает learning_rate на 0.1 каждые 7 эпох (это одна из эвристик, не было на лекциях)\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T11:56:07.781351Z",
     "start_time": "2021-04-25T11:56:07.767477Z"
    }
   },
   "outputs": [],
   "source": [
    "model_AlexNet = model_AlexNet.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T12:41:19.649245Z",
     "start_time": "2021-04-25T11:56:12.932213Z"
    }
   },
   "outputs": [],
   "source": [
    "history = train(train_dataset, val_dataset, model=model_AlexNet, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T12:41:29.968928Z",
     "start_time": "2021-04-25T12:41:29.960926Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T12:41:39.161195Z",
     "start_time": "2021-04-25T12:41:39.019163Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Feature Extractor** способ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:24:31.565822Z",
     "start_time": "2021-04-23T12:24:30.663618Z"
    }
   },
   "outputs": [],
   "source": [
    "model_AlexNet_extractor = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:24:33.625288Z",
     "start_time": "2021-04-23T12:24:33.611284Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model_AlexNet_extractor.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:24:35.750766Z",
     "start_time": "2021-04-23T12:24:35.721760Z"
    }
   },
   "outputs": [],
   "source": [
    "# замораживаем параметры (веса)\n",
    "for param in model_AlexNet_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 9216\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_AlexNet_extractor.classifier = nn.Linear(num_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T12:25:18.688458Z",
     "start_time": "2021-04-23T12:25:18.677455Z"
    }
   },
   "outputs": [],
   "source": [
    "model_AlexNet_extractor =model_AlexNet_extractor.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:07:36.404730Z",
     "start_time": "2021-04-23T12:25:20.718917Z"
    }
   },
   "outputs": [],
   "source": [
    "history = train(train_dataset, val_dataset, model=model_AlexNet_extractor, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:07:38.503204Z",
     "start_time": "2021-04-23T13:07:38.489201Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:07:41.052780Z",
     "start_time": "2021-04-23T13:07:40.915749Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Смешанный** способ:\n",
    "Мы будем обучать не только последний **fully connected** слой, но и несколько предпоследних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T21:27:41.831397Z",
     "start_time": "2021-04-24T21:27:41.178259Z"
    }
   },
   "outputs": [],
   "source": [
    "model__AlexNet_mixed = models.alexnet(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T21:27:42.006437Z",
     "start_time": "2021-04-24T21:27:41.992433Z"
    }
   },
   "outputs": [],
   "source": [
    "model__AlexNet_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T21:27:47.370155Z",
     "start_time": "2021-04-24T21:27:47.353152Z"
    }
   },
   "outputs": [],
   "source": [
    "layers_to_unfreeze = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T21:27:47.954225Z",
     "start_time": "2021-04-24T21:27:47.929219Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model__AlexNet_mixed.features[:-layers_to_unfreeze].parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "num_features = 9216\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model__AlexNet_mixed.classifier = nn.Linear(num_features,  n_classes)\n",
    "\n",
    "model__AlexNet_mixed =model__AlexNet_mixed.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T21:27:48.902190Z",
     "start_time": "2021-04-24T21:27:48.518103Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = train(train_dataset, val_dataset, model=model__AlexNet_mixed, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:50:41.974616Z",
     "start_time": "2021-04-23T13:50:41.837585Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T13:50:44.593207Z",
     "start_time": "2021-04-23T13:50:44.547197Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model__AlexNet_mixed.state_dict(), 'model__AlexNet_mixed.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-24T19:22:35.171944Z",
     "start_time": "2021-04-24T19:22:35.153940Z"
    }
   },
   "outputs": [],
   "source": [
    "model__AlexNet_mixed.load_state_dict(torch.load('model__AlexNet_mixed.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T12:41:21.789458Z",
     "start_time": "2021-04-25T12:41:21.774451Z"
    }
   },
   "source": [
    "## Подготовка Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:37:04.381553Z",
     "start_time": "2021-04-25T18:37:02.668171Z"
    }
   },
   "outputs": [],
   "source": [
    "model_inception_v3 = models.inception_v3(pretrained=True)\n",
    "model_inception_v3.aux_logits=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:37:04.749372Z",
     "start_time": "2021-04-25T18:37:04.735408Z"
    }
   },
   "outputs": [],
   "source": [
    "for param in model_inception_v3.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:37:05.149159Z",
     "start_time": "2021-04-25T18:37:05.134156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle the auxilary net\n",
    "num_ftrs = model_inception_v3.AuxLogits.fc.in_features\n",
    "model_inception_v3.AuxLogits.fc = nn.Linear(num_ftrs, n_classes)\n",
    "# Handle the primary net\n",
    "num_ftrs = model_inception_v3.fc.in_features\n",
    "model_inception_v3.fc = nn.Linear(num_ftrs,  n_classes)\n",
    "\n",
    "print(model_inception_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:37:07.635678Z",
     "start_time": "2021-04-25T18:37:06.429365Z"
    }
   },
   "outputs": [],
   "source": [
    "model_inception_v3 = model_inception_v3.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T18:37:10.010171Z",
     "start_time": "2021-04-25T18:37:10.006170Z"
    }
   },
   "outputs": [],
   "source": [
    "# все изображения будут масштабированы к размеру 299x299 (особенность, так обучался inception)\n",
    "RESCALE_SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:07:52.687080Z",
     "start_time": "2021-04-25T18:37:10.865267Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = train(train_dataset, val_dataset, model=model_inception_v3, epochs=30, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:09:55.136849Z",
     "start_time": "2021-04-25T20:09:54.970812Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###   ИТОГ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:09:59.141746Z",
     "start_time": "2021-04-25T20:09:59.130742Z"
    }
   },
   "outputs": [],
   "source": [
    "model_current=model_inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:09:59.871908Z",
     "start_time": "2021-04-25T20:09:59.860906Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_one_sample(model, inputs, device=DEVICE):\n",
    "    \"\"\"Предсказание, для одной картинки\"\"\"\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        logit = model(inputs).cpu()\n",
    "        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:01.408252Z",
     "start_time": "2021-04-25T20:10:01.359242Z"
    }
   },
   "outputs": [],
   "source": [
    "random_characters = int(np.random.uniform(0,1000))\n",
    "ex_img, true_label = val_dataset[random_characters]\n",
    "probs_im = predict_one_sample(model_current, ex_img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:02.542506Z",
     "start_time": "2021-04-25T20:10:02.052396Z"
    }
   },
   "outputs": [],
   "source": [
    "idxs = list(map(int, np.random.uniform(0,1000, 20)))\n",
    "imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n",
    "\n",
    "probs_ims = predict(model_current, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:04.598965Z",
     "start_time": "2021-04-25T20:10:04.584963Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:05.585186Z",
     "start_time": "2021-04-25T20:10:05.460158Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(probs_ims,-1)\n",
    "\n",
    "actual_labels = [val_dataset[id][1] for id in idxs]\n",
    "\n",
    "preds_class = [label_encoder.classes_[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:06.429375Z",
     "start_time": "2021-04-25T20:10:06.414372Z"
    }
   },
   "outputs": [],
   "source": [
    "actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:08.119753Z",
     "start_time": "2021-04-25T20:10:08.109751Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:09.093971Z",
     "start_time": "2021-04-25T20:10:09.084971Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:11.842586Z",
     "start_time": "2021-04-25T20:10:11.826582Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(actual_labels, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:14.171106Z",
     "start_time": "2021-04-25T20:10:12.828806Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n",
    "                        sharey=True, sharex=True)\n",
    "for fig_x in ax.flatten():\n",
    "    random_characters = int(np.random.uniform(0,1000))\n",
    "    im_val, label = val_dataset[random_characters]\n",
    "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
    "                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
    "    \n",
    "    \n",
    "\n",
    "    imshow(im_val.data.cpu(), \\\n",
    "          title=img_label,plt_ax=fig_x)\n",
    "    \n",
    "    actual_text = \"Actual : {}\".format(img_label)\n",
    "            \n",
    "    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n",
    "    font0 = FontProperties()\n",
    "    font = font0.copy()\n",
    "    font.set_family(\"fantasy\")\n",
    "    prob_pred = predict_one_sample(model_current, im_val.unsqueeze(0))\n",
    "    predicted_proba = np.max(prob_pred)*100\n",
    "    y_pred = np.argmax(prob_pred)\n",
    "    \n",
    "    predicted_label = label_encoder.classes_[y_pred]\n",
    "    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n",
    "    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n",
    "            \n",
    "    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n",
    "                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:38.762606Z",
     "start_time": "2021-04-25T20:10:31.762041Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = SimpsonsDataset(test_files, mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
    "probs = predict(model_current, test_loader)\n",
    "\n",
    "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "test_filenames = [path.name for path in test_dataset.files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:39.616798Z",
     "start_time": "2021-04-25T20:10:39.601795Z"
    }
   },
   "outputs": [],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:42.201376Z",
     "start_time": "2021-04-25T20:10:41.800286Z"
    }
   },
   "outputs": [],
   "source": [
    "# ДОБАВЛЕНО: создание сабмита\n",
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['Id'] = test_filenames\n",
    "df['Expected'] = preds\n",
    "df.to_csv('simpsons_simple_cnn_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T20:10:43.205600Z",
     "start_time": "2021-04-25T20:10:43.193598Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PYTORCH]",
   "language": "python",
   "name": "conda-env-PYTORCH-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
